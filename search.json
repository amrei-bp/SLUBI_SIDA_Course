[
  {
    "objectID": "Day4Session1_results.html",
    "href": "Day4Session1_results.html",
    "title": "RNASeq Results",
    "section": "",
    "text": "In this session, we will have a look at some of the results that were produced with the nf core pipelines we ran. It’s all good and well running analyses, but you need to be able to do something with the output.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "RNASeq Results"
    ]
  },
  {
    "objectID": "Day1Session1_nfcore.html",
    "href": "Day1Session1_nfcore.html",
    "title": "Nextflow and nf-core",
    "section": "",
    "text": "When developing your code in bioinformatics, you will likely use different tool for different parts of your analyses. Traditionally, you would have about one script per tool, all of which you deploy by hand, one after the other. Together, this is called a workflowor pipeline.\nManual deployment of pipelines can be tedious, especially if you have analyses with many steps, or many samples of different sizes that might need a varying amount of computational power. Luckily for you, other bioinformaticians and software developers have developed something to make your life much easier:",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Nextflow and nf-core"
    ]
  },
  {
    "objectID": "Day1Session1_nfcore.html#workflow-managers",
    "href": "Day1Session1_nfcore.html#workflow-managers",
    "title": "Nextflow and nf-core",
    "section": "Workflow managers",
    "text": "Workflow managers\n\nWorkflow managers provide a framework for the creation, execution, and monitoring of pipeline. &lt;…&gt; They simplify pipeline development, optimize resource usage, handle software installation and versions, and run on different compute platforms, enabling workflow portability and sharing. Wratten et al. (2021) Nature Methods\n\nWith workflow managers you can develop an automated pipeline from your scripts that can then be run on a variety of systems. Once it is developed, execute a single command to start the pipeline. The manager then coordinates the deployment of the scripts in the appropriate sequence, monitors the jobs, handles the file transfers between scripts, gathers the output, and handles re-execution of failed jobs for you. Workflow managed pipelines can run containers, which eliminates software installation and version conflicts.\nThat means that by design the pipelines are:\n\nportable\nmore time efficient (no more downtime between pipeline steps)\nmore resource efficient (mostly, but this might vary depending how skilled a developer you yourself are)\neasier to install (especially when combined with containers, or environment managers)\nmore reproducible\n\nThere are in principle two different flavors of workflow managers: snakemake, and nextflow. In this course we will be introducing you to nexflow.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Nextflow and nf-core"
    ]
  },
  {
    "objectID": "Day1Session1_nfcore.html#nextflow",
    "href": "Day1Session1_nfcore.html#nextflow",
    "title": "Nextflow and nf-core",
    "section": "Nextflow",
    "text": "Nextflow\nIn nextflow, your scripts are turned into processes, connected by channels that contain the data - input, output etc. The order of the processes, and their interaction with each other, is specfied in the workflow scope.\n\n\n\nsource: Software carpentries\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn this course, we will not write our own processes, or pipeline. However, if you are interested, there are a lot of very good training materials available online.\n\n\nThe executable part of the processes, the so called script, can be written in any language, so in theory you could always choose the language that is best suited for the job (in practice you might be limited to the languages you know). However, the modularity of the processes allows for easy re-use of existing scripts and processes.\nWhen moving the pipeline from one system to another, the script stays the same and does not change, the same containers are used. The only thing that changes are the parameters that pertain to the environment and available resources. In other words, with nextflow, the functional logic, the processes, are separated from the executive (how the workflow runs). This makes the nextflow pipelines highly interoperable and portable. They can be run on various platforms, such as HPC clusters, local computers, cloud systems etc..\nThe pipelines can be integrated with version control tools, such as git or bitbucket, and containers technologies, such as apptainer or docker. This makes the pipeline very reproducible.\nThe nextflow pipelines are extremely scalable, can be developed on a few samples and easily be run on hundreds or thousands of samples. When possible, processes are run in parallel automatically.\nNexflow performs automatic checks on the processes and their in- and output. It can automatically resume execution at a point of failure without having to re-compute successfully completed parts.\nNextflow is open source.\nHere is a more visual summary of some of the points above:\n\n\n\nsource: Maxime U Garcia, Seqera labs\n\n\nOne reason why we are using, and promoting, nexflow is the community project, nf-core",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Nextflow and nf-core"
    ]
  },
  {
    "objectID": "Day1Session1_nfcore.html#nf-core",
    "href": "Day1Session1_nfcore.html#nf-core",
    "title": "Nextflow and nf-core",
    "section": "nf-core",
    "text": "nf-core\n\n\n\nsource: nf-core\n\n\nNf-core is a very active community around nextflow. Volunteers develop nextflow pipelines around a variety of bioinformatic data.\nHere are some flagship pipelines that have been developed by the nf-core community (we will have a look at the entire list on the nf-core homepage in a bit):\n\n\n\nsource: nf-core\n\n\nAll nf-core pipelines are open source and the source code is available on github. The pipelines are developed by volunteers, who can have a very varied background.\n\n\n\n\n\n\nNote\n\n\n\nWhile nf-core is fantastic, please be aware that their pipelines are developed and maintained (or not maintained) by the community. You should not use the pipelines as a black box, but as a tool you need to understand. The responsibility for the end results is still yours, so you need to see if your data is suited for the analysis (good enough quality?), and if the analysis is suitable for your data!\n\n\nHowever, nf-core does not only develop pipelines.\nThe community also develops: - processes that they make available as modules (and optimizes then too). - training material for all user classes. - best practices for documentation. - templates for development.\nThere is a weekly online helpdesk, and even a podcast.\n\nWhat makes nf-core pipelines interesting for you?\nNf-core provides already developed pipelines for many different data sets. Likely, a pipelines exists that you can use on your data. The documentation of the pipelines follows nf-core guidelines and is extensive and informative - it is easy to understand what the pipeline does and how it works. All output is explained in detail, with links to more extensive documentation.\nOnce you have understood how to run a nf-core pipeline their consistency and standardization means you will know most about running a different one.\nUsing the nf-core launcher, will check your input, and automatically generate commands and configuration files.\nAnd this is on top of all nextflow functionality such as portability, reproducibility and the resume-at-fault option!",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Nextflow and nf-core"
    ]
  },
  {
    "objectID": "Day2Session2_nextflow_handson.html",
    "href": "Day2Session2_nextflow_handson.html",
    "title": "nfcore RNASeq Pipeline",
    "section": "",
    "text": "Now that you know about Linux, containers, pixi, and Nextflow, we get to start with the really cool part of our course! In this section, we will create a pixi environment containing nf-core and nextflow. Once we’ve done that, we will turn our attention to nf-core to set up the rnaseq pipeline. Finally, we will run the pipeline.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "nfcore RNASeq Pipeline"
    ]
  },
  {
    "objectID": "Day2Session2_nextflow_handson.html#introduction",
    "href": "Day2Session2_nextflow_handson.html#introduction",
    "title": "nfcore RNASeq Pipeline",
    "section": "",
    "text": "Now that you know about Linux, containers, pixi, and Nextflow, we get to start with the really cool part of our course! In this section, we will create a pixi environment containing nf-core and nextflow. Once we’ve done that, we will turn our attention to nf-core to set up the rnaseq pipeline. Finally, we will run the pipeline.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "nfcore RNASeq Pipeline"
    ]
  },
  {
    "objectID": "Day2Session2_nextflow_handson.html#setting-up-your-pixi-environment",
    "href": "Day2Session2_nextflow_handson.html#setting-up-your-pixi-environment",
    "title": "nfcore RNASeq Pipeline",
    "section": "Setting Up Your Pixi Environment",
    "text": "Setting Up Your Pixi Environment\nIn our course directory execute these commands, one after the other.\nLet’s inititalise an environment for this. Again, please substitute your name in the name part of the commands.\npixi init name_nextflow -c conda-forge -c bioconda\nChange directory into the project you created, and just list the files there\ncd name_nextflow\nls\nAdd nf-core and Nextflow\npixi add nextflow nf-core\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhile apptainer is sticky loaded on this server, it won’t always be the case for other servers. So, if you are running within a Linux environment (and not otherwise), you can add apptainer with the add command.\n\n\n\nAnd just check that everything worked, summon the help message from nf-core\npixi run nf-core --help",
    "crumbs": [
      "Home",
      "Hands-on training",
      "nfcore RNASeq Pipeline"
    ]
  },
  {
    "objectID": "Day2Session2_nextflow_handson.html#preparing-the-run",
    "href": "Day2Session2_nextflow_handson.html#preparing-the-run",
    "title": "nfcore RNASeq Pipeline",
    "section": "Preparing The Run",
    "text": "Preparing The Run\nFrom the nf-core homepage, we can search for pipelines. We are going to demo the rnaseq pipeline. You can see that there is a lot going on in this pipeline! We will chat more about these things in class rather than including screenshots of everything.\n\n\n\nrnaseq landing page\n\n\nUnder the Usage, you will find a lot of information that describes the pipeline, including input information, the samplesheet.csv (this one we will make together in class).\nUnder the Parameters tab you will find information on all of the things that we will set up in the next step.\nUnder the Output tab, you will find information on the expected output generated from the pipeline. This is useful to help you interpret what the pipeline produces.\nTo set up our own analysis, we will click on the launch version 3.20.0 button. (This was the version on the website at the time of writing this session. The version number may change). We are then redirected to a page where we can fill in all of our information about input files, as well as selecting or deselecting certain parts of the pipeline. We will share the things here that you need to input each time, and go through some finer details based on the discussion with you.\n\nSetting working and results directories\n\n\n\n\n\n\nImportant\n\n\n\nWe recommend that you use absolute paths rather than relative paths for setting up your runs.\n\n\nDuring the first part, you need to set a working and result directory. If you are using a server that has a profile established, you can put the name of the server there. If not, we will create our own configuration profile if we run into memory issues.\n\n\n\nSetting work and output directories\n\n\n\n\n\nSetting results and input CSV\n\n\nWe will compile the input CSV together in class. This is entirely unique to each analysis.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo list all fastq files with their absolute path, one per line use:\nfind . -maxdepth 1 -type f -name \"*.fastq.gz\" -exec realpath {} \\;\nYou can substitute the . that indicates it’s only looking in the directory you’re currently in to any other path on your file system.\n\n\n\n\nConfiguration profiles\nSince we are working on a server with a configuration profile established, we have downloaded it and put it in the course folder. If you want to fetch it for yourself\nwget https://raw.githubusercontent.com/hpc2n/intro-course/master/exercises/NEXTFLOW/INTERACTIVE/hpc2n.config\nHere is the configuration profile on HPC2N from the above link. The most important things we need to pay attention to are the max_memory, max_cpus, and max_time settings. If you want to create your own profile, you can adjust these to suit your system requirements.\n// Config profile for HPC2N\nparams {\n  config_profile_description = 'Cluster profile for HPC2N'\n  config_profile_contact = 'Pedro Ojeda @pojeda'\n  config_profile_url = 'https://www.hpc2n.umu.se/'\n  project = null\n  clusterOptions = null\n  max_memory = 128.GB\n  max_cpus = 28\n  max_time = 168.h\n  email = 'pedroojeda2011@gmail.com'\n}\n\nsingularity {\n  enabled = true\n}\n\nprocess {\n  executor = 'slurm'\n  clusterOptions = { \"-A $params.project ${params.clusterOptions ?: ''}\" }\n}\n\n\n\n\n\n\nNote\n\n\n\nslurm is a job management tool installed on many servers to distribute resources evenly among users.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you copy this config file for yourself, you need to remove the process section, unless you have a slurm job manager installed on your cluster. If you have this, you will defiinitely have a system administrator who can help you write this block to suit your system!\n\n\n\n\n\nSetting all other inputs that are required\nIn this section, you set variables that are related to your reference genome. If you are using something listed on iGenomes, you can input that name. If you are working with your own reference genome, or something not listed, you need to input the absolute path of the reference genomes you have downloaded.\n\n\n\nReference genome options\n\n\nDepending on your strategy, you might need to input a corresponding gff as well. It really depends on the kind of analysis you are hoping to perform.\n\n\nObtaining your JSON file\nOnce everything is filled in, click on Launch and you will be redirected to another page containing your JSON file that has information on your run. You can either run the analyses by copying the command at the top of the page (BUT DON’T PRESS ENTER JUST YET) or by copying the JSON file a bit lower on the screen and saving it as nf-params.json in your folder on HPC2N.\nYou then have to add one more row to the file, specifying the projects compute project, so the computation time can be debited from the correct account.\n    \"project\": \"account_name\"",
    "crumbs": [
      "Home",
      "Hands-on training",
      "nfcore RNASeq Pipeline"
    ]
  },
  {
    "objectID": "Day2Session2_nextflow_handson.html#starting-the-run",
    "href": "Day2Session2_nextflow_handson.html#starting-the-run",
    "title": "nfcore RNASeq Pipeline",
    "section": "Starting The Run",
    "text": "Starting The Run\n\nsubmit directly via pixi\nNow you can run the pipeline with the following command:\npixi run nextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json -c hpc2n.config\nThere are several layers to this command:\nFirst we invoke Pixi and tell it to run the following commands.\nThen we say which program we want to run, namely Nextflow.\nThe following commands are Nextflow/ nf-core commands:\n\nwe want to run the nf-core/rnaseq pipeline, version 3.19.0\nwe want to use the parameter file called nf-params.json\nwe want to use the hpc configuration file called hpc2n.config\n\n\n\nsubmit via sbatch\nAlternatively, you can run nextflow via pixi using a batch script and slurm: copy the following text to file called name_submit_rnaseq.sh where name is your name.\n#!/bin/bash -l\n#SBATCH -A our_proj_allocation\n#SBATCH -n 5\n#SBATCH -t 24:00:00\n\n/your_home_directory/.pixi/bin/pixi run nextflow run nf-core/rnaseq -r 3.19.0 -params-file /your_path/nf-params.json\nAnd then submit it to slurm with\nsbatch name_submit_rnaseq.sh\nYou can check the progress of your job with squeue -u your_username\nAnd now we wait until the run is done!\n\n\n\n\n\n\nTip\n\n\n\nNextflow is notoriously bad at cleaning after itself. You can check previous runs with pixi run nextflow log. And then clean up with for example pixi run nextflow clean -f -before &lt;run_name&gt;. Here is an explanation of the command.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "nfcore RNASeq Pipeline"
    ]
  },
  {
    "objectID": "Day4Session3_r_in_quarto.html",
    "href": "Day4Session3_r_in_quarto.html",
    "title": "Using R in Quarto",
    "section": "",
    "text": "As you’ve seen, Quarto is a really neat tool to integrate different programming languages, aggregate analyses, and write reports. One of the most commonly used languages in bioinformatics is R, and we wanted to show you some neat tips and tricks that we use often to make things a bit easier. Of course, please have a look at the R for Data Science book for more practical tips.\nWhat we are showing you here works just as well with R Markdown, so if you are already using that, or would prefer to work in the R GUI, please feel free to do so.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Using R in Quarto"
    ]
  },
  {
    "objectID": "Day4Session3_r_in_quarto.html#introduction",
    "href": "Day4Session3_r_in_quarto.html#introduction",
    "title": "Using R in Quarto",
    "section": "",
    "text": "As you’ve seen, Quarto is a really neat tool to integrate different programming languages, aggregate analyses, and write reports. One of the most commonly used languages in bioinformatics is R, and we wanted to show you some neat tips and tricks that we use often to make things a bit easier. Of course, please have a look at the R for Data Science book for more practical tips.\nWhat we are showing you here works just as well with R Markdown, so if you are already using that, or would prefer to work in the R GUI, please feel free to do so.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Using R in Quarto"
    ]
  },
  {
    "objectID": "Day4Session3_r_in_quarto.html#code-in-r",
    "href": "Day4Session3_r_in_quarto.html#code-in-r",
    "title": "Using R in Quarto",
    "section": "Code in R",
    "text": "Code in R\nIn R, you have the option of working in the console, in an R script or in an R Markdown (among many options). Within the console, you execute code bit by bit. Within an R script you can execute code line-by-line, in sections, or as a whole. In R Markdown, you create a chunk that you can also execute line-by-line or as a whole block. In Quarto, you will use chunks as well.\nA chunk is a bit of code surrounded by backticks. A single backtick highlights things like this. A chunk is started with 3 backticks, filled with code, and closed with 3 backticks. You also indicate the language you will code in with curly brackets and the language you want to use.\n\n\n\n\n\n\nTip\n\n\n\nIf you add a fullstop in front of the language, your code is shown but it is not executed {r} would run an R chunk, but {.r} would not\n\n\n\nSuppressing warnings\nR produces a lot of warnings! Sometimes these warning are useful, but most of the time, they aren’t useful for us. If we look back at yesterday’s example of running R in Quarto:\n\npacman::p_load(tidyverse, palmerpenguins)\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThere are 2 warnings about removing 2 rows. There are also some warnings about the libraries.\nTo remove these for a whole document, you can add this to the title section of your page to apply only to the page it is on (the title would be your title, this title is just this page’s title):\ntitle: \"Using R in Quarto\"\nwarning: false\n---\nYou can also silence warnings for particular blocks (because sometimes you might care about the warnings!) In that case, you would add this in the first line of your chunk:\n#| warning: false\n\npacman::p_load(tidyverse, palmerpenguins)\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Using R in Quarto"
    ]
  },
  {
    "objectID": "Day4Session3_r_in_quarto.html#useful-packages-we-love",
    "href": "Day4Session3_r_in_quarto.html#useful-packages-we-love",
    "title": "Using R in Quarto",
    "section": "Useful Packages We Love!",
    "text": "Useful Packages We Love!\nThere are plenty of really cool packages out there, but the 2 that stand out to us to help with project management and organisation, and keep your environment easy to use are pacman and here\n\npacman\npacman is a package manager that checks whether you have a library installed before loading it with the p_load function that you can see in the chunks above. If you don’t have the package installed, it checks several repositories, installs it, and loads it. If you have it installed, it simply loads it. You can install particular versions with it, install GitHub packages, unload particular packages, as well as temporarily installing packages. This has made life with R a lot easier, especially if you are supporting users/students across multiple versions of R!\n\n\nhere\nhere is a smoother way of setting working directories in R. It sets the working directory relative to a file that is pointed to with the here::i_am(\"file\") command. This is incredibly useful when you are sharing projects and scripts between different people. As long as you are sharing the whole folder and don’t change the architecture (like renaming files and folders), the script will work the same on each system- no more manually changing the setwd() command!",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Using R in Quarto"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html",
    "href": "Day5Session1_AI.html",
    "title": "Caution: AI in Bioinformatics",
    "section": "",
    "text": "Caution\n\n\n\nWhile the lure of AI is becoming more present in our daily lives, remember that you do not need it. Before AI, you were perfectly able to design a packing list for your upcoming trip. You were able to look at a paper to find answers to your scientific questions. You knew how to query a vignette in R to determine how a function should be used.\nLife was a bit slower, but you used your mind and your agency. You made decisions. Please do not confuse convenience with need!",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html#introduction",
    "href": "Day5Session1_AI.html#introduction",
    "title": "Caution: AI in Bioinformatics",
    "section": "Introduction",
    "text": "Introduction\nIn the past 3 years, AI has become more mainstream. The primary kind of AI people think of, is a Large Language Model (LLM). At this point, you cannot really avoid contact with these models anymore as they have infiltrated every aspect of the internet.\n\n\n\nSearching LLM with Google to be answered with an LLM response\n\n\nThese algorithms are language models trained on incredibly large datasets. They are trained to recognise and generate natural language. The chatbots we are all familiar with are generative pretrained transformers (GPTs). These can be trained for specific tasks, or guided by prompt generation.\nThis is not an AI theory course, and we really do not have enough knowledge to explain the underlying theory beyond a rudimentary level. We would, however, like to discuss the impact of AI on you as a user, and advocate for responsible use of AI in your current and future work.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html#machine-learning-in-bioinformatics",
    "href": "Day5Session1_AI.html#machine-learning-in-bioinformatics",
    "title": "Caution: AI in Bioinformatics",
    "section": "Machine Learning in Bioinformatics",
    "text": "Machine Learning in Bioinformatics\nMachine learning (ML) has been used in bioinformatics and other fields of data science for many decades on every level. Before ML, algorithms had to be programmed by hand rather than having the algorithms learn features of a dataset. With ML, features of a dataset can be annotated based on previously annotated datasets. These algorithms were a mix of supervised (learning on annotated data) and unsupervised (learning on unannotated data) learning, depending on the function of the algorithm.\nSupervised algorithms are used for classification and regression analyses. Unsupervised algorithms are used to discover hidden patterns in data without needing a human’s input. Unsupervised algorithms are used in clustering, association, and dimensionality reduction\n\n\n\n\n\n\nNote\n\n\n\n\n\nClassification: Output is a discrete variable. Linear classifiers, support vector machines, decision trees, random forests. E.g. annotating a new genome based on genome annotations from existing species.\nRegression: Focus on understanding dependent and indepedent variables.\n\n\n\nFor more info, see here, and here.\nThere is no arguing that these algorithms have led to great progress within the field of bioinformatics. Generative AI is one of the next steps in the evolution of applying machine learning in our lives.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html#incorporation-of-ai-in-our-lives",
    "href": "Day5Session1_AI.html#incorporation-of-ai-in-our-lives",
    "title": "Caution: AI in Bioinformatics",
    "section": "Incorporation of AI in Our Lives",
    "text": "Incorporation of AI in Our Lives\nChatGPT gained 100 million users in the 2 months after its release in 2022, making it the fastest-growing consumer application in history. Generative AI (GenAI) models now come in many different flavours, depending on the developer.\n\n\n\nGenAI chatbots by market share in August 2025 from FirstPageSage\n\n\n\nTraining Data\nAs with earlier iterations of supervised and unsupervised algorithms, GenAI models have all been trained on existing data. And this existing data can be biased: in a historical context, history was recorded by the party that won the war. History changed as different empires and narratives rose and fell. With digitisation, this information has landed on the internet. In the more modern “Internet Age”, everyone with an internet connection can technically post anything they’d like on the internet. This can add different types of biases - not everyone has equal access to the internet, some people may not have strong enough opinions to post about something online, some people prefer to read rather than contribute, while others take pleasure in “shit-posting”.\nAs the GenAI training data contains large amounts of data from the internet, these differences in how people use the internet have an effect on how useful the trained models become. In this example, you can clearly see what the effect of bad training data is:\n\n\n\nGenAI answer to a simple question\n\n\nFurthermore, GenAI’s are not programmed to say that they do not know something, and will happily hallucinate an answer. If you do not know better, or trust the computers, you may take a made-up answer as true and post it elsewhere. As the use of AI’s increases, AI generated content is used to train new AI’s. Gary Illyes from Google has spoken about “human curated” vs “human created” data being used as training data.\nIn a perfect world, GenAI would be trained on perfectly curated data, but even with all of the data that we have on the internet at the moment, we do not have nearly enough data. We have to make do with what we have.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html#some-words-of-caution",
    "href": "Day5Session1_AI.html#some-words-of-caution",
    "title": "Caution: AI in Bioinformatics",
    "section": "Some Words of Caution",
    "text": "Some Words of Caution\nGenAI is becoming more integrated in every sector of our lives. It is important that we use the new technology responsibly. When Google first came out, there were classes on how to use the search engine, determine validity of sources and information, and how to stay safe on the internet. This section aims to raise awareness about commonly overlooked aspects of GPT use.\n\nLearning with AI\nAI has great potential in the field of education. ChatGPT has been shown to be highly beneficial in an educational environment when integrated properly. However, the use of AI in this setting must be balanced and carefully curated. A 2025 pre-print by Kosmyna et al showed that adults that used ChatGPT to write SAT type essays were outperformed consistently by adults that wrote the same essay without the support of an AI, and had significantly lower brain engagement.\n\n\nProductivity with AI\nA recent study by a non-profit group, Model Evaluation and Threat Research (METR) aimed to quantify the difference in productivity when using AI. Participants in this study were not new to their field, with at least 5 years of experience prior to this study being conducted.\n\n\n\nAI reducing productivity\n\n\nThe study also found that when AI is allowed, the participants spent less time coding and seeking solutions to the problems. Rather, they spent time prompting the AI, reading and reviewing responses, and being idle. Intel produced similar findings.\n\n\n\nReasons for loss of productivity with AI\n\n\n\n\nData Privacy and Legal Concerns\nData privacy concerns have been present throuhout the development and use of AI.\nThe use of copyrighted material in training AI, and its legality, is being discussed and debated in several courts. The questions around using text and data mining are divisive with some parties arguing that finding patterns, trends, and insights in existing data being how new research is done by humans and should be extended to AI, while others, particularly in the European context, disagree to some extent. Understanding the legality of the service you use in different countries falls on you as a user.\nThe Terms of Service (ToS) of different GPTs are important when deciding whether to use a GPT at all. For example, Deepseek’s ToS (collected on 15.08.2025) states:\nAccount Personal Data. We collect Personal Data that you provide when you set up an account, such as your date of birth (where applicable), username (where applicable), email address and/or telephone number, and password.\n\nUser Input. When you use our Services, we may collect your text input, prompt, uploaded files, feedback, chat history, or other content that you provide to our model and Services (“Prompts” or \"Inputs\"). We generate responses (“Outputs”) based on your Inputs.\n\nPersonal Data When You Contact Us. When you contact us, we collect the Personal Data you send us, such as proof of identity or age, contact details, feedback or inquiries about your use of the Services or Personal Data about possible violations of our Terms of Service (our “Terms”) or other policies.\nBiological data enjoys a high level of protection, and is often considered as highly sensitive. Too often, users will input sequences, tables, or other data into a GPT to find ways to plot data, perform sequence annotations, or similar tasks. Be wary of doing this. Check the ToS explicitly, and frequently. Try to use GDPR compliant GPT’s if you must use a GPT. Try using only a description of your data - such as column names and type of data - instead of the actual data set.\nAI as browser extensions steal deeply personal information when enabled in a users’ browser including financial, education, and medical information, whether an extension is being actively used or not. The authors have also commented on how these practices interfere with the company’s own ToS as well as privacy legislation.\n\n\nPersonal Responsibility\nYou as a user are responsible for the AI generated content you choose to use. If you, for instance, ask AI to generate a brand logo for you that is too close to something that exists already, the original owner is free to sue you as an individual for copyright infringement.\nAs scientists, we know that we should use peer-reviewed resources whenever possible. It is why we cannot cite Wikipedia in a scientific article. GPT’s have been widely shown to fabricate citations based on how it has learnt a citation should look. It is up to you as a user to check every single citation that AI generates since you are responsible for what you write.\n\n\nGlobal Linguistic Changes\nEven though ChatGPT has only been widely used for 3 years, it has already started leaving its traces in how we speak. Words like delve and meticulous are being used more frequently in academic YouTube talks.\n\n\n\nGPT words in YouTube videos from Yakura et al 2025\n\n\nIt has also been shown that different GPT’s have different writing styles, also known as idiolects.\nSome projects like this one are trying to customize GPT ideolects to match writing styles of unique users. This will make detecting AI use more difficult in future.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis course was written by two people, and each person wrote their own sections without the use of any AI. Can you tell who wrote which sections based on idiolects?\n\n\n\nWe know that subtle linguistic shifts can change emotional regulation within individuals. We also know from sociolinguistics, that even the slightest linguistic features can serve to bind or divide us.\n\n\nAI on the Internet\nSocial media platforms are an important aspect in the development, improvement, and implementation of GenAI models. OpenAI, the creators of ChatGPT, have used a subreddit on the social media platform, Reddit, to train their new algorithm. Google and OpenAI have contractual agreements with Reddit to license data from the users on the platform. Earlier in 2025, researchers from the University of Zurich were implicated in an experiment on the same subreddit OpenAI used to train a model. They wanted to test whether an interaction with a bot was more likely to make people change their minds than an interaction with a real person. This was heavily frowned upon, and posts were all removed as users had no ability to consent to participating in a study. It has also been suggested that interactions observed by the researchers were just bots arguing with each other.\nA preprint released in February 2025 by Liang et al. found that the amount of content generated by AI rose from 2-3% in November 2022 to 24% by the end of 2023.\n\n\n\nAI slop trough by Yahoo! News!\n\n\n\n\nEnvironmental Impact\nThe facilities to run GenAI require a significant amount of resources. These facilities need a huge amount of electricity to power the facility (this places extreme strain on exising infrastructure and increases the grid’s carbon footprint) as well as water to cool the hardware. Currently, data centers use more electricity than many independent countries.\nSome data centers are being built near poor communities, drain resources from, and add pollution to the community (see Colossus that has been built in Memphis to power the X bot, Grok for an example. Musk is not the only offender.)\nWhile we cannot do anything about where data centers are built, we can make informed decisions about which platforms we use. We can also be careful with the number of queries we send, and how we use our queries. In April 2025, the CEO of OpenAI said that polite requests like “please” and “thank you” have cost tens of millions of dollars due to the cost of electricity.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html#how-to-decide-when-to-use-ai",
    "href": "Day5Session1_AI.html#how-to-decide-when-to-use-ai",
    "title": "Caution: AI in Bioinformatics",
    "section": "How To Decide When To Use AI",
    "text": "How To Decide When To Use AI\nIf we know the risks and the true cost of what we are doing, we can make informed decisions about how we chose to incorporate new technologies into our day-to-day and working lives.\nHere are some questions that we find useful to ask ourselves before opening a GPT:\n\nAm I phrasing my prompt in a good way? Here is a guide to prompt engineering that might be useful.\nCan I find this information any other way?\nHow much time am I saving by looking this question up here vs on BioStars, for example?\nDo I know enough about the topic to know whether the GPT is lying to me?\nWhat are the consequences of testing the validity of the GPT solution? Can I potentially corrupt my data or my system? Is there a potential for me to lie to someone who trusts me enough to ask my opinion?\n\nIf a GPT is used to learn a new skill, remember how important active learning is. Seek explanations for everything the GPT tells you. Find independent sources that were produced by experts to validate your learning.\nHold on to the ability to learn and the desire to be curious.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day1Session2_linux.html",
    "href": "Day1Session2_linux.html",
    "title": "A Brief Introduction to Linux",
    "section": "",
    "text": "Unix-like operating systems are built under the model of free and open-source development and distribution. They often come with a graphical user interface (GUI) and can be run from the command line (CLI) or terminal. The CLI is a text-based interface that works exactly the same way as you would use your mouse, but you use words. It can be intimidating at first, but once you have mastered the basics, it’s really not different than using your mouse!\nIt is important to know how to use the terminal as all servers, and most bioinformatics tools do not have a GUI and rely on the use of the terminal.\n\n\n\n\n\n\nWarning\n\n\n\nMacOS and Linux, and Windows have significant differences in their syntax. Where we do things locally, we will point out some of the differences, but for the most part, we will provide Windows users with a local Linux platform so that this course can run as smoothly as possible\n\n\nFor this course, we do not expect you to be masters of Linux, but we will need some knowledge of how to find files, and some other basic Linux commands.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "A Brief Introduction to Linux"
    ]
  },
  {
    "objectID": "Day1Session2_linux.html#introduction",
    "href": "Day1Session2_linux.html#introduction",
    "title": "A Brief Introduction to Linux",
    "section": "",
    "text": "Unix-like operating systems are built under the model of free and open-source development and distribution. They often come with a graphical user interface (GUI) and can be run from the command line (CLI) or terminal. The CLI is a text-based interface that works exactly the same way as you would use your mouse, but you use words. It can be intimidating at first, but once you have mastered the basics, it’s really not different than using your mouse!\nIt is important to know how to use the terminal as all servers, and most bioinformatics tools do not have a GUI and rely on the use of the terminal.\n\n\n\n\n\n\nWarning\n\n\n\nMacOS and Linux, and Windows have significant differences in their syntax. Where we do things locally, we will point out some of the differences, but for the most part, we will provide Windows users with a local Linux platform so that this course can run as smoothly as possible\n\n\nFor this course, we do not expect you to be masters of Linux, but we will need some knowledge of how to find files, and some other basic Linux commands.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "A Brief Introduction to Linux"
    ]
  },
  {
    "objectID": "Day1Session2_linux.html#filesystem-architecture",
    "href": "Day1Session2_linux.html#filesystem-architecture",
    "title": "A Brief Introduction to Linux",
    "section": "Filesystem Architecture",
    "text": "Filesystem Architecture\nLinux uses a hierarchical filesystem, similar to Windows and Mac. In this figure from TecAdmin, there is a representation of this.\n\n\n\nFilesystem\n\n\nWe can see here that the root or / folder is at the top of the hierarchy, with all other folders, like home/ and var/ inside of it.\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe use / at the end of a folder name to show that it is a folder.\n\n\n\nThere are two different ways for us to know where our file is within the operating system. The first is the absolute path and the second is the relative path. The absolute path gives us information that is true anywhere on your operating system. Whether your terminal is open in /usr/bin/something/ or /var/tmp/, a file will always be located at /usr/Documents/sequence.fasta as it is the true or absolute location. The relative path, as the name suggests, is relative to where you currently are on the file tree. If your terminal is open in /usr/Documents/paper/figures/, the file from before, sequence.fasta will be two folders up from where you are. If you were in /var/bin/something/ it would three folder up, one to the side, and two folders down.\n\n\n\n\n\n\nTip\n\n\n\n\n\nIt is often good practice to use absolute paths when you set pipelines up to run- this way, your programs know where to go looking for the files they’re supposed to work on",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "A Brief Introduction to Linux"
    ]
  },
  {
    "objectID": "Day1Session2_linux.html#connecting-to-a-server",
    "href": "Day1Session2_linux.html#connecting-to-a-server",
    "title": "A Brief Introduction to Linux",
    "section": "Connecting to a Server",
    "text": "Connecting to a Server\nThere are many different ways you can connect to a server with SSH. At SLUBI, we have a strong preference towards Visual Studio Code or its open-source alternative, VSCodium. It provides a graphical user interface where you can create, edit, and view files, and see the file system.\nTo be able to connect to a server, you need to install an extension called Remote-SSH from the extensions market place.\n\n\n\nExtensions from View\n\n\n\n\n\nExtensions from the shortcut\n\n\nNow we need to add a host to our list of known hosts. There are several ways of doing this, and all ways lead to establishing a connection.\n\nUnder the previously shown View option, navigate to the Command Palette. A dropdown menu at the top will start with a &gt;. Type Remote-SSH and select add new host. Here we will input our credentials. This will populate a file in a hidden folder, .ssh in your local home directories called known_hosts.\nYou can also write all of these lines manually. This way, you can also specify SSH keys that may be needed to log on to particular servers. We won’t be covering that in this course, though.\n\nAfter a host has been added, you can connect to it. You can do this either through the Command Palette, or through the small blue backwards and forwards arrows in the bottom left corner of VSCode. You will receive a list of known hosts that you can connect to. If you need to connect with a password, a password prompt will be shown.\nOnce connected, you can open the file manager on the left.\nFor privacy concerns, we will share the exact commands locally in the room.\n\n\n\n\n\n\nImportant\n\n\n\nRemeber to only open either your home directories or the folder for your projects with the file manager. If you try to open folders that are too large, you will cause VSCode to crash (and you may cause significant problems for your system administrator!)",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "A Brief Introduction to Linux"
    ]
  },
  {
    "objectID": "Day1Session2_linux.html#navigating-with-the-terminal",
    "href": "Day1Session2_linux.html#navigating-with-the-terminal",
    "title": "A Brief Introduction to Linux",
    "section": "Navigating with the terminal",
    "text": "Navigating with the terminal\nFor this part of the session, we will do some hands-on practice! For this, we will open a terminal. You can do this through the Command Palette by using the View: Toggle Terminal function, or clicking the box with a dark bottom half and an empty top half in the top right corner of VSCode.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor Windows Users If you are using a Windows operating system, you can install the free Home edition of MobaXterm when your access to this server is removed if you want to continue practicing your Linux skills.\nTo use MobaXterm as a Linux shell, click on New Session and select a bash shell. There will probably be a prompt that you have to installl an extension. Click on the link it will provide, or copy the extension it needs and search it on Google (or your preferred search engine) and install it. Then try starting a new session again.\n\n\n\nTo see where we currently are in the filesystem, we use the present working directory command. This will give us the absolute path of the directory that our terminal open in.\npwd\nTo see the files that are currently in the directory we use the list command.\nls\nTo make a directory, we use the make directory command, followed by the name we would like our directory to have. Here we will make a directory called sida_training.\nmkdir sida_training\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhen we give files and folders names, we don’t use spaces or special characters. It makes it really difficult to access files. We can use different ways to name our files and folders 1. youcanbechaoticandusenothing 2. YouCanUseCapitalLetters 3. you_can_use_underscores 4. you-can-use-dashes The most important thing is to be consistent with what you use, and to use descriptive names that are not too long. Future you will thank past you!\n\n\n\nTo enter the sida_training directory, we will use the change directory command.\ncd sida_training\nWe can use the pwd command again just to make sure that we are indeed in the right folder.\nTo go back to the folder we were in previously, we can use a shortcut rather than the absolute path.\ncd ..\n\n\n\n\n\n\nTip\n\n\n\n\n\nThere are no limits to the amount of directories you can go back (provided they are in your filesystem). If you wanted to go up 3 directories, you’d use\ncd ../../..\n\n\n\nTo delete files and folder you use the remove command. If you want to remove a folder, you need a recursive flag.\nrm -r sida_training\nIf we had a file to look at, we could use the less command. In the interface, we press q to quit.\nless filename\nWith these Linux basics, you will certainly be able to follow our course. If you would like to learn more, Data Carpentry and Software Carpentry have excellent tutorials on using the terminal in more detail, and we can only recommend them!",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "A Brief Introduction to Linux"
    ]
  },
  {
    "objectID": "Day1Session3_pixi.html",
    "href": "Day1Session3_pixi.html",
    "title": "Introduction to Pixi",
    "section": "",
    "text": "Different operating systems bring different challenges to bioinformatics. Windows, for instance, doesn’t really support most bioinformatics tools, or your computer might run a different version of a specific tool than mine. One way to solve this, and make research more reproducible, is through the use of environments. Bioinformatics environments specify the tools neded for the task at hand, and environment managers install these tools with all their dependencies. There are many different kinds of environment managers, and for this course we are going to use Pixi.\nThere are, of course, other ways to solve tool access and compatibility issues, such as running virtual machines. In our experience, however, environments are a bit easier to manage and are more portable across different systems and users.\n\n\n\nPixi landing page\n\n\nAs you can see, you can run Pixi on all major operating systems, and you can include various platforms in your Pixi environment.\n\n\nInstalling Pixi is really easy and described thoroughly here with separate installation guides for Windows and Mac/Linux.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo source your shell, you need to source the startup files, in Linux it’s the ~/.bashrc file, in Mac it’s the ~/.zshrc file.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Environment managers: Pixi"
    ]
  },
  {
    "objectID": "Day1Session3_pixi.html#introduction",
    "href": "Day1Session3_pixi.html#introduction",
    "title": "Introduction to Pixi",
    "section": "",
    "text": "Different operating systems bring different challenges to bioinformatics. Windows, for instance, doesn’t really support most bioinformatics tools, or your computer might run a different version of a specific tool than mine. One way to solve this, and make research more reproducible, is through the use of environments. Bioinformatics environments specify the tools neded for the task at hand, and environment managers install these tools with all their dependencies. There are many different kinds of environment managers, and for this course we are going to use Pixi.\nThere are, of course, other ways to solve tool access and compatibility issues, such as running virtual machines. In our experience, however, environments are a bit easier to manage and are more portable across different systems and users.\n\n\n\nPixi landing page\n\n\nAs you can see, you can run Pixi on all major operating systems, and you can include various platforms in your Pixi environment.\n\n\nInstalling Pixi is really easy and described thoroughly here with separate installation guides for Windows and Mac/Linux.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo source your shell, you need to source the startup files, in Linux it’s the ~/.bashrc file, in Mac it’s the ~/.zshrc file.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Environment managers: Pixi"
    ]
  },
  {
    "objectID": "Day1Session3_pixi.html#setting-up-an-environment",
    "href": "Day1Session3_pixi.html#setting-up-an-environment",
    "title": "Introduction to Pixi",
    "section": "Setting Up An Environment",
    "text": "Setting Up An Environment\nYou should create separate environments for each project you run, just to keep things tidy. To create an environment, you have to specify a name for your environment. You can include different platforms/operating systems in your environment, for example if you want to develop your code on a Windows system, and later use the same code on many samples on a large Linux cluster. You can also include different vetted sources for your tools, the so called channels.\nHere, we will create a project called name_sida_training (please use your own name to avoid creating multiple environments with the same name). We are adding the conda-forge and bioconda channels with the -c flag.\npixi init name_sida_training -c conda-forge -c bioconda\nPixi will create a folder named name_sida_training with a a file pixi.toml. Let’s have a look at that file!\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere is the code for changing directories, listing files, and viewing the contents of a file:\ncd name_sida_training\nls\nless pixi.toml\nTo exit the less view, press q for quit.\n\n\n\n\npixi.toml\nThe .toml file give your information about your project. Let’s have a look at one I made one my Mac before adding any dependencies to my environment. How is it different from the one you’ve made on HPC2N?\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"sida_quarto\"\nplatforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\n \n\n\n\n\n\n\nNote\n\n\n\nYour current platform will be automatically detected and added to the environment. If you want to add different platforms, you add them with the -p linux-64 flag. In this example, you are adding Linux64. See the Pixi docs for the full list of supported platforms.\nIf you are adding a platform that doesn’t natively run on your OS (e.g. adding Linux when running on Windows) be sure to add the OS you are running your system on as well!\n\n\nOnce you have used the environment, or added a dependency/tool to it, you will find yet another file, called pixi.lock.\n\n\npixi.lock\nThe .lock file give you information on the channels you have decided to add, as well as the information on where the packages were downloaded from, license information, md5 information, and more.\n\n\n\n\n\n\nImportant\n\n\n\nDo not delete the .toml or .lock files, or you will break your environment!",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Environment managers: Pixi"
    ]
  },
  {
    "objectID": "Day1Session3_pixi.html#adding-dependencies",
    "href": "Day1Session3_pixi.html#adding-dependencies",
    "title": "Introduction to Pixi",
    "section": "Adding Dependencies",
    "text": "Adding Dependencies\nAdding dependencies to the .toml file is telling Pixi to install the specified program for you. However, instead of installing it globally, it only gets installed in the environment.\nTo do this, we use the pixi add function. Let’s try adding Quarto to our environments (we will use Quarto extensively in this course!)\n\n\n\n\n\n\nImportant\n\n\n\nYou must be in the folder of the project to add software!\n\n\npixi add quarto\n\n\n\n\n\n\nTip\n\n\n\n\n\nIf you are unsure of how a function works, you can always query it, usually with the --help or -h flags. Here’s how it would look for the pixi add function\npixi add --help\nA general rule of thumb is that a single hyphen - is followed by a single letter flag, while double hyphens -- are usually followed by multi-letter flags\n\n\n\nHere is the pixi.toml we’ve seen earlier after I have added Quarto to my environment. You can see that the dependencies have been updated to include Quarto.\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"sida_quarto\"\nplatforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\nquarto = \"&gt;=1.7.32,&lt;2\"",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Environment managers: Pixi"
    ]
  },
  {
    "objectID": "Day1Session3_pixi.html#running-a-package",
    "href": "Day1Session3_pixi.html#running-a-package",
    "title": "Introduction to Pixi",
    "section": "Running a Package",
    "text": "Running a Package\nNow that we have an environment with a tool installed, we actually want to use it. For this we use the pixi run function.\nLet’s query the help function within Quarto:\npixi run quarto --help\n\n\n\n\n\n\nNote\n\n\n\nTo use other tools, simply substitute quarto with the package you’ve added, and the tool specific commands.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Environment managers: Pixi"
    ]
  },
  {
    "objectID": "Day3Session2_quarto.html",
    "href": "Day3Session2_quarto.html",
    "title": "Introduction to Markdown and Quarto",
    "section": "",
    "text": "Markdown is a lightweight markup language for text editing. All of the documents you have seen in this course have been formatted with markdown. The platform this course website has been built on is called Quarto and implements markdown. In this section, we’ll look at some of the post commonly used markdown syntax, using Quarto to create lab notebooks, presentations, automate analyses, and host websites like this one.\nRecently, a lot of open-source books like R for Data Science and Python for Data Science have been created with Markdown using Quarto. The code for these books have also been publicly released.\n\n\n\n\n\n\nImportant\n\n\n\nPlease bookmark these books for your students! They are excellent resources!",
    "crumbs": [
      "Home",
      "Data management",
      "Introduction to Markdown and Quarto"
    ]
  },
  {
    "objectID": "Day3Session2_quarto.html#introduction",
    "href": "Day3Session2_quarto.html#introduction",
    "title": "Introduction to Markdown and Quarto",
    "section": "",
    "text": "Markdown is a lightweight markup language for text editing. All of the documents you have seen in this course have been formatted with markdown. The platform this course website has been built on is called Quarto and implements markdown. In this section, we’ll look at some of the post commonly used markdown syntax, using Quarto to create lab notebooks, presentations, automate analyses, and host websites like this one.\nRecently, a lot of open-source books like R for Data Science and Python for Data Science have been created with Markdown using Quarto. The code for these books have also been publicly released.\n\n\n\n\n\n\nImportant\n\n\n\nPlease bookmark these books for your students! They are excellent resources!",
    "crumbs": [
      "Home",
      "Data management",
      "Introduction to Markdown and Quarto"
    ]
  },
  {
    "objectID": "Day3Session2_quarto.html#markdown-syntax",
    "href": "Day3Session2_quarto.html#markdown-syntax",
    "title": "Introduction to Markdown and Quarto",
    "section": "Markdown Syntax",
    "text": "Markdown Syntax\nThere are plenty of really great cheatsheets like this one from Markdown Guide as well as this one. We’ll go through some of these details with you live rather than copying and pasting really good tutorials here.",
    "crumbs": [
      "Home",
      "Data management",
      "Introduction to Markdown and Quarto"
    ]
  },
  {
    "objectID": "Day3Session2_quarto.html#using-markdown",
    "href": "Day3Session2_quarto.html#using-markdown",
    "title": "Introduction to Markdown and Quarto",
    "section": "Using Markdown",
    "text": "Using Markdown\nThere are many different platforms that use Markdown syntax in text editing. At SLUBI, we have a strong preference towards Visual Studio Code or its open-source alternative, VSCodium. To use all of the features, you need to install the Markdown extension.\nTo access to Extensions, either navigate with shortcuts (these are operating system depdenent and can be set by users, so we won’t go into that, but feel free to explore), from the View dropdown menu, or from the Extensions tab on the left.\n\n\n\nExtensions from View\n\n\n\n\n\nExtensions from the shortcut\n\n\nIn the search bar, you can search for general Markdown viewers and install them. For this course, we will use Quarto. Install that extension. Please also note the verified tick.\n\n\n\nQuarto extension to install\n\n\nThe Markdown All in One extension is also an interesting case to look at. It has a lot of downloads despite not being verified by an organisation. In general, be wary when installing extensions. Opt for ones that are installed often, as this one is. If possible, try to use verified extensions. In the age of information security we live in, it is becoming more important to use trusted content than whatever you find on the internet.",
    "crumbs": [
      "Home",
      "Data management",
      "Introduction to Markdown and Quarto"
    ]
  },
  {
    "objectID": "Day3Session2_quarto.html#quarto",
    "href": "Day3Session2_quarto.html#quarto",
    "title": "Introduction to Markdown and Quarto",
    "section": "Quarto",
    "text": "Quarto\nQuarto is an open-source technical and scientific reporting and publishing system.\n\n\n\nQuarto landing page\n\n\n\nInstalling Quarto on your Machine\nWe have already installed the extension that allows us to use a bunch of really cool features in VSCode with Quarto, but we need to have the backend installed on our machines as well. To install the program, you will click on the Get Started link on the homepage, download the program that’s suited to your operating system, and install it. The Guide link here also takes you to really easy tutorials on how to set up a variety of other uses of Quarto that we simply don’t have time for in this course.\n\n\n\n\n\n\nImportant\n\n\n\nAll instructions for creating, editing, previewing, and rendering projects will be shown in VSCode and it will not be specified over and over\n\n\n\n\nCreating a Project\nUnder the previously shown View option, navigate to the Command Palette. A dropdown menu at the top will start with a &gt;. Type quarto and a list of functions will be shown. The order will be different based on what you’ve used and what you’ve used most recently.\n\n\n\nQuarto functions\n\n\nWe will Create a project and create a Website for the purpose of this course.\n\n\n\nCreating a Project\n\n\n\nStructure of a Project\nAll Quarto projects have a YAML file with information about the project called _quarto.yml as well as an index.qmd file. The landing page of your website is the index.qmd file, while the _quarto.yml file contains all of the visual aspects of your website. We’ll spend a bit of time looking at the design of our course website so that you can see how it looks behind the scenes. Hopefully, with a website you know as well as you do by now, this will make things a lot more tangible!\n\n\n\n\n\n\nNote\n\n\n\n\n\nFun fact: YAML stands for “yet another markup language”\n\n\n\n\n\n\nAdding Files and Figures\nOften, we don’t just need a landing page like the index.qmd file, but we want different pages, as we have here. You can add blank documents with the .qmd extenstion. It is important for each file to have a title section so that it can be displayed properly and linked with your other pages. Update the _quarto.yml file to include this page in your table of contents or navigation bar (depending on what you opted to use), or simply link to the file with a hyperlink as we have done on our landing page\n\n\n\n\n\n\nTip\n\n\n\nRemeber to give your files a good name with no spaces in them. Also remember to add the .qmd extension\n\n\nTo add external figures to your document as we have done here with screenshots, you can simply link to the figure by its relative path, as described here.\n\n\n\n\n\n\nTip\n\n\n\nIt is good practice to have your images in a folder inside of your Quarto project in case Future You forgets that a figure is linked to a document and you move the folder containing all of your photos, for example. It also makes it much easier to put everything into a repository on GitHub to launch your site\n\n\n\n\nAdding Analyses\nQuarto is really good at implementing other programming languages within it. You can use R within it seamlessly. There is support for a large number of other programming languages within Quarto, but this course is only going to showcase the use of R.\n\n\n\n\n\n\nImportant\n\n\n\nYou will have to install the R and R syntax extensions for this to work in VSCode\n\n\n\npacman::p_load(tidyverse, palmerpenguins)\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIf you have analyses that you have to repeat across several projects, these documents can be shared to create uniformity among your produced results, and your students do not need to reinvent the wheel. With several pages being supported in a single Quarto project, you also reduce the potential messiness that is introduced whenever you onboard a new student.\n\n\nViewing the Project\n\nPreviewing\nAs you go along, you might be interested in seeing what your pages look like. In the terminal, type\nquarto preview\nor simply click the preview button in VSCode and a side by side preview will open where you can click through your indexed pages.\n\n\n\nVSCode Preview function\n\n\n\n\nDifferent Views\nFrom the Command Palette, you can select whether you would like to view your document in source mode or in visual mode. The difference is mostly that you can interact with the document in the way it will be rendered in visual mode. For some people, visual mode is a lot more intuative, and there is no right or wrong way to interact with your documents.\n\n\n\nPublishing and Sharing the Project\nWith Quarto, you can render to HTML, Word, and PDF formats. To render individual files into PDF, add the information from this Quarto guide into the header of your document.\n---\ntitle: \"My document\"\nformat:\n  pdf:\n    toc: true\n    number-sections: true\n    colorlinks: true\n---\nYou will have to install a recent distribution of TeX for this via your terminal\nquarto install tinytex\nFor this course, we are simply going to render our documents into HTML format. In the next section, we will cover how to host Quarto pages on GitHub.",
    "crumbs": [
      "Home",
      "Data management",
      "Introduction to Markdown and Quarto"
    ]
  },
  {
    "objectID": "Day2Session1_containers.html",
    "href": "Day2Session1_containers.html",
    "title": "Containers",
    "section": "",
    "text": "In an ideal world I would be able to write a piece of software, or a develop some code to analyse data on my computer, and then send someone else this software or code and they could run it as well, getting the same results.\nIn reality, I would very likely run into at least one of the following problems:\n\nIt is not uncommon for people within the same team to use different operating systems (whether MacOS, Windows, or different flavours of Unix builds). Even if everyone is using a MacOS, there are still different versions that impact the way people are able to work with their machines.\nAlmost every piece of software has some sort of dependency - other software - it needs to run. Some programs might “just” need a bash shell or basic python, while others need a variety of compilers and additional libraries to function. Often, these dependencies require further dependencies to be installed. It is also not uncommon for dependencies for Program 1 to clash with the dependencies for Program 2, requiring the user to uninstall dependencies to be able to install others.\nIn bioinformatics, tools are very often not maintained after the student that wrote the software graduated, the PI moved to a different university, or the funding simply ran out. This leads to a lot of really good software not really being supported by newer operating systems, usually due to dependencies not being easily available or, as before, clashing with newer versions. This makes installing a tool one of the biggest hurdles to overcome in bioinformatics.\nYou often cannot install different versions of the same program on one computer due to conflicting names. This is particularly problematic when you want to rerun an analysis for a publication where you need to use the same software all the way through.\n\nAny of these points might lead to you not being able to run my code, or it running but giving different results. They make bioinformatics less reproducible as tools and code cannot be moved easily betweem systems (for example if you upgrade your computer or want to share your pipeline with a colleague). Fortunately, most of these problems can be overcome with containers.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Containers"
    ]
  },
  {
    "objectID": "Day2Session1_containers.html#reproducibility-in-bioinformatics",
    "href": "Day2Session1_containers.html#reproducibility-in-bioinformatics",
    "title": "Containers",
    "section": "",
    "text": "In an ideal world I would be able to write a piece of software, or a develop some code to analyse data on my computer, and then send someone else this software or code and they could run it as well, getting the same results.\nIn reality, I would very likely run into at least one of the following problems:\n\nIt is not uncommon for people within the same team to use different operating systems (whether MacOS, Windows, or different flavours of Unix builds). Even if everyone is using a MacOS, there are still different versions that impact the way people are able to work with their machines.\nAlmost every piece of software has some sort of dependency - other software - it needs to run. Some programs might “just” need a bash shell or basic python, while others need a variety of compilers and additional libraries to function. Often, these dependencies require further dependencies to be installed. It is also not uncommon for dependencies for Program 1 to clash with the dependencies for Program 2, requiring the user to uninstall dependencies to be able to install others.\nIn bioinformatics, tools are very often not maintained after the student that wrote the software graduated, the PI moved to a different university, or the funding simply ran out. This leads to a lot of really good software not really being supported by newer operating systems, usually due to dependencies not being easily available or, as before, clashing with newer versions. This makes installing a tool one of the biggest hurdles to overcome in bioinformatics.\nYou often cannot install different versions of the same program on one computer due to conflicting names. This is particularly problematic when you want to rerun an analysis for a publication where you need to use the same software all the way through.\n\nAny of these points might lead to you not being able to run my code, or it running but giving different results. They make bioinformatics less reproducible as tools and code cannot be moved easily betweem systems (for example if you upgrade your computer or want to share your pipeline with a colleague). Fortunately, most of these problems can be overcome with containers.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Containers"
    ]
  },
  {
    "objectID": "Day2Session1_containers.html#containers",
    "href": "Day2Session1_containers.html#containers",
    "title": "Containers",
    "section": "Containers",
    "text": "Containers\n\nWhat are containers?\nContainers are stand-alone pieces of software that require a container management tool to run. They are build and exchanged as container images that specify the contents of the container, such as the operating system, all dependencies, and software in an isolated environment. The container management tool then takes the images and build the container. These management tools can be run on all operating systems, and since the container has the operating system within it, it will run the same in all environments. Container images are easily portable and immutable, so they are stable over time.\n\n\nRunning Containers\nThere are several programs that can be used to build and run containers. Docker, Appptainer, and Podman are the most commonly used platforms to date. They all have their pros and cons. If you are using a Windows machine that only you are using, then Docker is likely the least complex tool to install. On multi-user systems like a server, Apptainer is the best tool for the job. For this tutorial and the rest of the course, we will use Apptainer commands. There are small syntax changes between bash and powershell commands, but they are very similar.\n\n\nDownloading Container Images\nThere are several repositories for people to publish container images that they have specified. Dockerhub and Seqera are two commonly used platforms for downloading container images. You are able to use container images from dockerhub on Apptainer without any problems.\n\ndockerhub Tutorial\nOn the dockerhub landing page, you have a search bar, and some login options. You do not need to create an account to access the containers on dockerhub.\n\n\n\ndockerhub landing page\n\n\nFor these tutorials, we’ll search VCFtools, a commonly used software for VCF manipulation and querying. The results of the search give us several different containers with the same name.\n\n\n\nRegistry search\n\n\nYou can see who made the container image, how many times it has been downloaded (or pulled), when it was updated (here updated means different versions of the image being uploaded), and how many people have starred it. It is usually a good rule of thumb to use the most popular images from users that have uploaded a lot of container images. The biocontainers and pegi3s profiles have builds for a lot of tools, and they are built really well!\nIf we click on the vcftools from biocontainers we get to a typical dockerhub image landing page:\n\n\n\nVCFtools page\n\n\nThere is information on the frequency of the container image being pulled, as well as a pull command to download the image. This command is for docker, so we need to modify it for Apptainer.\napptainer pull vcftools_0.1.16-1.sif docker://biocontainers/vcftools:v0.1.16-1-deb_cv1\nThis command has several parts to it:\n\napptainer calls on the Apptainer software to run\npull tells Apptainer which function to use. In this case, we want it to go fetch something from a repository\nvcftools_0.1.16-1.sif is the name of the container image on our local machine. We could call it I_Love_Dogs but that is not very informative at all. Your collaborator won’t know what it means, and you certainly won’t know what it means in 6 months from now! It is also good practice to put the version number in your image name in case you want to have several versions at the same time, and you need to tell them apart.\ndocker:// is the registry you are pulling from. There are several different registries, but we are only going to show 2 during this course. (You will see another one in the Seqera tutorial)\nbiocontainers/vcftools is the profile/repository and container you are pulling\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nFile format extensions like .txt and .sif are really only important for us. However, it is good practice to append your files with appropriate extensions to ensure that you follow good data management practices\n\n\n\nIf you are interested in a different version than the current version, there are other versions under the tags tab:\n\n\n\nContainer versions\n\n\nIf you wanted to download another version of the container, you simply copy the command shown on the right side, and alter the syntax, for example\napptainer pull vcftools0.1.14.sif docker://biocontainers/vcftools:v0.1.14_cv2\n\n\nSeqera Tutorial\nThe Seqera landing page is a bit different from the dockerhub landing page, and it works a bit differently from dockerhub. Dockerhub hosts container images that users have uploaded, while Seqera makes container images as you request them. They use bioconda, conda-forge, and pypi libraries to build their containers images with Wave. The advantage is that you can include several different softwares in your container image at once. The disadvantage is that you are limited to software hosted on the aforementioned repositories. Usually this isn’t a problem, but sometimes you want to use something that isn’t hosted there.\n\n\n\nSeqera containers landing page\n\n\nWhen you pull an image from Seqera and want to run it with Apptainer, you need to remember to change the container setting from Docker to Singularity, the older name of Apptainer.\n\n\n\nSelecting Singularity\n\n\nSince Seqera builds containers on-demand, sometimes you have to wait for the container to finish compiling. You can see that it is still preparing the container image from the fetching container comment. Don’t try to pull it when it is still building!\n\n\n\nWaiting to build\n\n\nWhen the container image is ready, you can copy the text and pull the image to your system:\n\n\n\nReady to download\n\n\napptainer pull vcftools_0.1.17.sif oras://community.wave.seqera.io/library/vcftools:0.1.17--b541aa8d9f9213f9\nHere we use oras:// instead of docker:// as we are pulling from the oras registry. We are also pulling a different version from Seqera, so the name of the container is different.\n\n\n\nRunning Containers\nOnce you have the container image on your local machine, you want to be able to use it. Apptainer can be used to build the container from the image. Then you can either enter the container and run as if you had the exact same operating system as the person who built it, or you can run the software inside the container from outside of the container.\nThere are 2 different ways to use a container: run and exec. The apptainer run command launches the container and first runs the %runscript for the container if one is defined, and then runs your command (we will cover %runscript in the Building Containers section). The apptainer exec command will not run the %runscript even if one is defined. It is a small, fiddly detail that might be applicable if you use other people’s containers. After calling Apptainer and the run or exec commands, you can use your software as you usually would\napptainer exec vcftools_0.1.17.sif vcftools --version\nThis command runs your vcftools_0.1.17.sif container from the image, calls on the program vcftools that is within the container, and shows you the version. If you had installed VCFtools locally, you would have just used\nvcftools --version\n\n\n\n\n\n\nImportant\n\n\n\nPlease remember that VCFtools is just an example. If you want to run any other tool everything after apptainer run or apptainer exec has to be substituted by the name of the specific container image and the run commands for that particular tool!\n\n\n\n\nBuilding Containers\nIf the software you would like to use is not packaged into a container by anyone else, you might want to build it yourself. For this, we are just going to show a very simple example. Building containers from scratch is a computationally intensive task. You build containers from a definition file with the extension .def\nHere we are going to build a container with a cow telling us the date. Save this in a file called lolcow.def.\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    date | cowsay | lolcat    \nThere are several components to this definition file.\n\nYou can set the operating system you want in the container, in this case Ubuntu 20.04.\n%post section is where you update the OS from its base state, install dependencies and so on.\n%environment is where you export paths and modify the environment.\n%runscript is the script that will run when you use apptainer run container.sif. If you don’t include a runscript, then nothing will happen when you try to run it without any commands. You could build this container without anything in the %runscript section, and use apptainer run container.sif date | cowsay | lolcat to get the same output.\n\napptainer build lolcow.sif lolcow.def\nYou’ll get a lot of output on the status of the build, ending in\nINFO:    Adding environment to container\nINFO:    Adding runscript\nINFO:    Creating SIF file...\nINFO:    Build complete: lolcow.sif\nWe can now run our new container with\napptainer run lolcow.sif\n\n\n\nBoring cow\n\n\n\n\n\n\n\n\nNote\n\n\n\nTry removing the %runscript, build it again, and see what happens.\n\n\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    fortune | cowsay | lolcat    \nIf we use the same definition file as before, but substitute date for fortune in the runscript and build the container, we now get a philosophical cow with a dark sense of humour:\n\n\n\nFun cow\n\n\n\n\n\nInspirational cow\n\n\nTo show the difference between the run and exec commands, we can use the same container with fortune in the runscript and run:\napptainer run lolcow.sif bash -c \"date|cowsay\"\nand\napptainer exec lolcow.sif bash -c \"date|cowsay\"\nThe run command gives us a philosophical cow while exec gives us our boring cow again.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Containers"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This course is a hosted by the Swedish Agricultural University’s Bioinformatics Infrastructure (SLUBI) team. In this course we hope to give you information on how to use reproducible bioinformatics pipelines, report results in a streamlined manner, and implement the system in your own research groups.\nThis course is funded by SIDA and is hosted in Alnarp, Sweden on the 25th-29th August 2025"
  },
  {
    "objectID": "Day4Session2_use_containers.html",
    "href": "Day4Session2_use_containers.html",
    "title": "Using Containers",
    "section": "",
    "text": "The aim of this session is to provide you with support if you would like to try to download and run your own containers. If there are common points we missed in our introductory session, or simply common questions, we will add these here.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Using Containers"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html",
    "href": "Day3Session1_data_management.html",
    "title": "Data Management for Reproducible Research",
    "section": "",
    "text": "When working with any type of data, it makes sense to sit down before the project starts to think through the different life stages of the data in your project. This will help counteract some of the problems that can arise when projects grow more organically, and will help consistency within the research group, ease of collaboration, and mostly your future self that will understand what past-self has been up to in the project.\n\n\n\n\n\n\nNote\n\n\n\nMore and more funding agencies expect a Data Management Plan at some point of a project application. In there, you need to document that you have thought of, and planned for, the life cycle of your data.\n\n\n\n\n\nThe Research Data Management toolkit for Life Sciences",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#data-life-cycle",
    "href": "Day3Session1_data_management.html#data-life-cycle",
    "title": "Data Management for Reproducible Research",
    "section": "",
    "text": "When working with any type of data, it makes sense to sit down before the project starts to think through the different life stages of the data in your project. This will help counteract some of the problems that can arise when projects grow more organically, and will help consistency within the research group, ease of collaboration, and mostly your future self that will understand what past-self has been up to in the project.\n\n\n\n\n\n\nNote\n\n\n\nMore and more funding agencies expect a Data Management Plan at some point of a project application. In there, you need to document that you have thought of, and planned for, the life cycle of your data.\n\n\n\n\n\nThe Research Data Management toolkit for Life Sciences",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#fair-principles",
    "href": "Day3Session1_data_management.html#fair-principles",
    "title": "Data Management for Reproducible Research",
    "section": "FAIR principles",
    "text": "FAIR principles\nIn the past, research data was often generated with one question in mind. Often, they would afterwards land in some drawer and be forgoten about. Nowadays researchers acknowledge that data can also be re-used, or combined with other data, to answer different questions.\nThe FAIR principles promote efficient data discovery and reuse by providing guidelines to make digital resources:\n\n\n\nWilkinson et al. (2016)\n\n\nFAIR principles, in turn, rely on good data management practices in all phases of research:\n\nResearch documentation\nData organisation\nInformation security\nEthics and legislation",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#reproducible-research",
    "href": "Day3Session1_data_management.html#reproducible-research",
    "title": "Data Management for Reproducible Research",
    "section": "Reproducible research",
    "text": "Reproducible research\nLucky for us, once we implement good data management practices, we will also increase the reproducibility of our analyses. Extensive documentation will increase faith in the outcome of analyses, and will help people (again, future-you) understand what has been done.\nLast, but not least, reproducible research practices make project hand-overs smoother, when the next person already understands the structure of the project, and can rely on good documentation.",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#what-data-do-we-work-with",
    "href": "Day3Session1_data_management.html#what-data-do-we-work-with",
    "title": "Data Management for Reproducible Research",
    "section": "What data do we work with?",
    "text": "What data do we work with?\n\nBioinformatics is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex. (Wikipedia)\n\nThis data can come from a variety of different biological processes:\n\n\n\nsource: Lizel Potgieter\n\n\nEarly on, sequencing data was not readily available, but due to decreasing costs and increased computational power biological data is now being produced in ever increasing quantities:\n\n\n\nGrowth of the Sequence Read Archive, SRA, from 2012 to 2021\n\n\nAt the same time, new technologies are being developed, and new tools that might or might not be maintained or benchmarked against existing tools. It’s the wild west out there!\n\n\n\nOverview of modern sequencing technologies and where they apply to biological processes",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#ways-to-work-on-data",
    "href": "Day3Session1_data_management.html#ways-to-work-on-data",
    "title": "Data Management for Reproducible Research",
    "section": "Ways to work on data",
    "text": "Ways to work on data\n\nThe solution is to approach bioinformatics as a bioinformatician does: try stuff, and assess the results. In this way, bioinformatics is just about having the skills to experiment with data using a computer and understanding your results. source: Vince Buffalo\n\nThis is a traditional way to work with bioinformatics data, and can still have its merits. However, in this course we would like to introduce you to a more structured way to make sense of your data.\nLet’s have a look at how a beginning PhD student might approach their data:\n\nThey might analyse their data, and get some results.\nAfter talking with their supervisor they might get a few other samples from a collaborator, or need to drop them from the analyses due to quality concerns.\nThey run the analyses again and get a different set of results.\nThere might be a few iterations of this process, and then the reviewers require some additional analyses…\n\nIn the “end” we have something like this:\n\n\n\nWhich one of these is the latest version?\n\n\n\n\n\n\n\n\nBest practices file organization\n\n\n\n\nThere is a folder for the raw data, which does not get altered.\nCode is kept separate from data.\nUse a version control system (at least for code) – e.g. git.\nThere should be a README in every directory, describing the purpose of the directory and its contents.\nUse file naming schemes that makes it easy to find files and understand what they are (for humans and machines) and document them.\nUse non-proprietary formats – .csv rather than .xlsx",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#literate-programming",
    "href": "Day3Session1_data_management.html#literate-programming",
    "title": "Data Management for Reproducible Research",
    "section": "Literate programming",
    "text": "Literate programming\nOur hypothetical PhD student, even if taking into account the best practice tips from above, is still likely to run the same analyses over and over whenever the input data changes. Sometimes, this might be months, or even years, after the original analysis was performed.\nLuckily for our student, they can save their code snippets (with intuitive file names) and re-use the code from back then. This is often done with R-scripts, but can just as well be applied to bash scripts, python scripts etc.\n\n\n\n\n\n\nWrite Code for Humans, Write Data for Computers\n\n\n\n\nCode should be readable, broken down into small contained components (modular), and reusable (so you’re not rewriting code to do the same tasks over and over again). These practices are crucial in the software world, and should be applied in your bio‐ informatics work as well.\n\n\nIn contrast to code, data should be formatted in a way that facilitates computer read‐ ability. All too often, we as humans record data in a way that maximizes its readability to us, but takes a considerable amount of cleaning and tidying before it can be pro‐ cessed by a computer. The more data (and metadata) that is computer readable, the more we can leverage our computers to work with this data.\n\nVince Buffalo\n\n\nIn the past years, the development went even further and one can even combine code and documentation in the same document. The code is wrapped in so called chunks, that are executable from within the document.\nThese notebooks come in different flavors, for example jupyter notebooksand marimo for Python applications, Rmarkdown for R code. Its successor, quarto can be used to integrate a variety of coding languanges. In this course, we will introduce you to quarto.\n\n\n\n\n\n\nMake Figures and Statistics the Results of Scripts\n\n\n\n\nWriting scripts to produce images and tables may seem like a more time-consuming process than generating these interactively in Excel or R. However, if you’ve ever had to regenerate multiple figures by hand after changing an earlier step, you know the merit of this approach. Scripts that generate tables and images can easily be rerun, save you time, and lead your research to be more reproducible.\n\nVince Buffalo",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#hands-on-quarto",
    "href": "Day3Session1_data_management.html#hands-on-quarto",
    "title": "Data Management for Reproducible Research",
    "section": "Hands-on: quarto",
    "text": "Hands-on: quarto\nWith this, we can turn to the quarto hands-on training to learn more about quarto and its capabilities.",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#version-control",
    "href": "Day3Session1_data_management.html#version-control",
    "title": "Data Management for Reproducible Research",
    "section": "Version control",
    "text": "Version control\nNow that our student has reproducible documents, with reasonable names, that can execute their analyses reliably over and over again, what happens if they modify their analyses? Will they end up again with different result files and their project sink down in chaos?\nNo, because there is version control, the practice of tracking and managing changes to files.\nVersion control can be used on the local system, where both the version database and the checked out file - the one that is actively being worked on - are on the local computer. Good, but the local computer can be corrupted and then the data is compromised.\n\n\n\nLocal version control\n\n\nVersion control can also be centralized, where the version database is on a central server, and the active file can be checked out from several different computers. This is useful when working from different systems, or when working with collaborators. However, when the central servers is compromised the historical version are lost.\n\n\n\nCentralized version control\n\n\nAt last, version control can be fully distributed, with all versions of the file being on the server and different computers. Each computer checks out the file from its own version database to work on them. The databases are then synchronized between the different computers and the server. One such distributed version control system is git. It can handle everything from small to very large projects and is simple to use. GitHubis a code hosting platform for version control and collaboration, built on git.\n\n\n\nDistributed version control\n\n\nDistributed version control facilitates collaboration with others. Software like git automatically tracks differences in files, and flags conflicts between files.\nAdditionally, GitHub, the code hosting platform based on git that we are using in this course, can be used to maintain uniformity within a working group. The group can develop their own project template that people can use and populate for their own projects.",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#hands-on-git-and-github",
    "href": "Day3Session1_data_management.html#hands-on-git-and-github",
    "title": "Data Management for Reproducible Research",
    "section": "Hands-on: git and github",
    "text": "Hands-on: git and github\nWe have prepared a hands-on training for git and GitHub for you.",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#environment-managers",
    "href": "Day3Session1_data_management.html#environment-managers",
    "title": "Data Management for Reproducible Research",
    "section": "Environment managers",
    "text": "Environment managers\nUsing git, our PhD student can now share their reproducible code with their colloaborators, or between systems. They can rest assured that the different versions of the notebook are tracked and can be checked out when necessary. But what about the bioinformatic tools?\nDifferent computers can run on different operating systems, or can have different versions of databases installed. This can lead to conflicts between tools, or software versions and can impact code usability, or reproducibility.\nFortunately, smart people have developed environment managers such as conda, bioconda, or pixi. These tools find and install packages, so that the same package versions are being run between different computers. However, the code might still give different results on different operating systems.",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#hands-on-managing-environments-with-pixi",
    "href": "Day3Session1_data_management.html#hands-on-managing-environments-with-pixi",
    "title": "Data Management for Reproducible Research",
    "section": "Hands-on: managing environments with pixi",
    "text": "Hands-on: managing environments with pixi\nHere is a link to our introduction to pixi.",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#containers-in-bioinformatics",
    "href": "Day3Session1_data_management.html#containers-in-bioinformatics",
    "title": "Data Management for Reproducible Research",
    "section": "Containers in bioinformatics",
    "text": "Containers in bioinformatics\nBut what if our PhD student needs to run their code on different operating systems?\nThey can use containers, that contain everything needed to run the application, even the operating system. Containers are being exchanged as container images, which makes them lightweight. Containers do not change over time, so the results will be the same today and in a few years. Everyone gets the same container that works in the same way.",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#hands-on-containers",
    "href": "Day3Session1_data_management.html#hands-on-containers",
    "title": "Data Management for Reproducible Research",
    "section": "Hands-on: containers",
    "text": "Hands-on: containers\nHere is our tutorial on where to get container images, and how to use - and even build your own - containers.",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#workflow-manager---nextflow",
    "href": "Day3Session1_data_management.html#workflow-manager---nextflow",
    "title": "Data Management for Reproducible Research",
    "section": "Workflow manager - Nextflow",
    "text": "Workflow manager - Nextflow\nNow our PhD student can use containers, or environments, to provide a uniform environment for their version controlled, wonderfully documented and reproducible code. Fantastic! But they still have to deploy, or at least monitor, their scripts manually.\nFortunately there are workflow managers that can integrate all of the above, submit your jobs for you, and even monitor and re-submit scripts after failure. They will automatically submit jobs for you, decreasing downtime and increasing efficiency.\n\n\n\n\n\n\nLet Your Computer Do the Work For You\n\n\n\n\nHumans doing rote activities tend to make many mistakes. One of the easiest ways to make your work more robust is to have your computer do as much of this rote work as possible. This approach of automating tasks is more robust because it decreases the odds you’ll make a trivial mistake such as accidentally omitting a file or naming out‐ put incorrectly.\n\nVince Buffalo",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#hands-on-nextflow",
    "href": "Day3Session1_data_management.html#hands-on-nextflow",
    "title": "Data Management for Reproducible Research",
    "section": "Hands-on: Nextflow",
    "text": "Hands-on: Nextflow\nFollow this link for a more detailed introduction to nextflow.",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "Day3Session1_data_management.html#things-to-take-home",
    "href": "Day3Session1_data_management.html#things-to-take-home",
    "title": "Data Management for Reproducible Research",
    "section": "Things to take home",
    "text": "Things to take home\nWe have now talked about many different aspects of data management, and tools and guidelines to make your life, and managing a research group, easier. Instilling best practices on students early on in their life will mean that routines will become second nature. We believen that making projects more FAIR and reproducible might initially take a bit more time, but will ultimately boost productivity.\nIf you cannot implement all of the above, choose some things that you can implement this time, and aim to implement another aspect the next time.",
    "crumbs": [
      "Home",
      "Data management",
      "Data Management for Reproducible Research"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SLUBI SIDA Course",
    "section": "",
    "text": "We are pleased to have this course with you! Our main aim with this course is to share how we use bioinformatics tools in a reproducible and scalable way. We will showcase the use of environments, containers, and established pipelines so that you can run these analyses on any operating system, as well as on systems that are not high performance computing clusters.\nOur approach to this course is going to be more practical and outcome based than theory based. We would like to give you the practical experience and skills to implement these kinds of analyses at your home universities. Our timeplan is also fairly flexible so that we can spend time with you on things that are useful and important for you to take hard skills with you. We can always go into more detail of something if you would like to know more about something.\nThe website will remain active after the course so that you have access to the material, and we encourage you to share these resources with your students and other researchers that might benefit from this!\n\nProgram\n\n\nDay\nSession\n\n\n\n\nMonday\nIntroduction to Linux\n\n\n\nIntroduction to Environments\n\n\n\nIntroduction to nf-core and Nextflow\n\n\nTuesday\nIntroduction to Containers\n\n\n\nSetting up and running a Nextflow Pipeline (with us)\n\n\n\nSetting up and running a Nextflow Pipeline (by yourself)\n\n\nWednesday\nData Management and Reproducible Research\n\n\n\nGitHub\n\n\n\nIntroduction to Markdown and Quarto\n\n\n\nHow do Nextflow Pipelines Work?\n\n\nThursday\nNextflow Results\n\n\n\nUsing Containers Outside of Nextflow\n\n\n\nUsing R and Other Languages in Quarto\n\n\nFriday\nAI in Bioinformatics\n\n\n\nWhat are your needs? How can you implement this in your own institutions?\n\n\n\nFriday is an emptier day in case we run out of time for something, or if there is something that you would like to learn more about.\nEvery day will end with a feedback session, and we hope that you will tell us what you like, what isn’t working for you, and what you would like to see more of.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Day2Session3_nextfow_own_choice.html",
    "href": "Day2Session3_nextfow_own_choice.html",
    "title": "Setting up a different nfcore pipeline",
    "section": "",
    "text": "For this section, we would like you to test out a different pipeline from nf-core that might be appropriate for your research. We are here to support you in this session.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Setting up a different nfcore pipeline"
    ]
  },
  {
    "objectID": "Day2Session3_nextfow_own_choice.html#things-to-think-about",
    "href": "Day2Session3_nextfow_own_choice.html#things-to-think-about",
    "title": "Setting up a different nfcore pipeline",
    "section": "Things to think about",
    "text": "Things to think about\n\nIs the pipeline maintained?\nWhat do you need as input?\nWhat are some of the constraints you might face during the setup?\n\n\n\n\n\n\n\nNote\n\n\n\nTo test the settings of a pipeline it is advisable to run the pipeline on only a few, reduced samples to save on computation time.\nOnce you are sure that the pipeline is set up correctly you can then run on the full data set.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Setting up a different nfcore pipeline"
    ]
  },
  {
    "objectID": "Day3Session3_github.html",
    "href": "Day3Session3_github.html",
    "title": "Version control: git and Github",
    "section": "",
    "text": "Git is a version control software that is fully distributed - meaning that each project folder contains the full history of the project. These project folders are also called repositories and can be on several computers, or servers.\nGithub is a code hosting platform that is based on git. Here you can store, track and publish code (and code only, do NOT use github for data!). On Github you can collaborate with colleagues and work on projects together.\nLet’s have a closer look at how git works:",
    "crumbs": [
      "Home",
      "Data management",
      "Version control: git and Github"
    ]
  },
  {
    "objectID": "Day3Session3_github.html#git",
    "href": "Day3Session3_github.html#git",
    "title": "Version control: git and Github",
    "section": "git",
    "text": "git\n\nGit has three main states that your files can reside in: modified, staged, and committed:\n\n\n\nModified means that you have changed the file but have not committed it to your database yet.\nStaged means that you have marked a modified file in its current version to go into your next commit snapshot.\nCommitted means that the data is safely stored in your local database.\n\n\n\nsource: git documentation\n\nThis leads to the three main sections of a Git project: the working directory, the staging area, and the Git directory (or repository).\n\n\n\nWorking directory, staging area, and Git directory\n\n\nAnd the basic commands of git:\n\n\n\nBasic git commands\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese basic operations are all done on your local system. You have the entire history of the project on your local disk, and do not need an internet connection to work on your data with git. You can do all your commits on your local computer and later push them to a remote repostitory, like Github.",
    "crumbs": [
      "Home",
      "Data management",
      "Version control: git and Github"
    ]
  },
  {
    "objectID": "Day3Session3_github.html#use-git-with-github",
    "href": "Day3Session3_github.html#use-git-with-github",
    "title": "Version control: git and Github",
    "section": "Use git with Github",
    "text": "Use git with Github\nTo get started with GitHub in VS Code, you’ll need to install Git, create a GitHub account and install the GitHub Pull Requests and Issues extension.\n\n\n\n\n\n\nNote\n\n\n\nAs of March 2023, GitHub required all users who contribute code on GitHub.com to enable one or more forms of two-factor authentication (2FA). Here is the step by step how-to enable two factor authentication.\nHere is another summary, plus suggestions which software to use.",
    "crumbs": [
      "Home",
      "Data management",
      "Version control: git and Github"
    ]
  },
  {
    "objectID": "Day3Session3_github.html#create-a-new-repository-on-github",
    "href": "Day3Session3_github.html#create-a-new-repository-on-github",
    "title": "Version control: git and Github",
    "section": "Create a new repository on Github",
    "text": "Create a new repository on Github\nGo to www.github.com, and sign in if you haven’t done so yet. On your profile page klick on the tab Repositories (if you are not on your profile you can navigate there by clicking on the icon in the top right corner).\nThen create a new repository by clicking on the green button New:\n\n\n\nnew repository\n\n\n\nChoose a repository name - there will be a green checkmark if the name is available.\n(optional) Add a short description of the repository contents.\nChoose if the repository should be public or private.\nAdd a README (remember, good data management practices?)\n\nClick on the green button Create repository to create repository.\n\n\n\n\n\n\nNote\n\n\n\nA public repository is visible to anybody. People can also copy clone the repository and then change code in their copy. However, they cannot change the code in your repository unless you give them permission or make them collaborators.\nA private repository is only visible by you and people you gave permission to.\n\n\n\n\n\nempty git repository",
    "crumbs": [
      "Home",
      "Data management",
      "Version control: git and Github"
    ]
  },
  {
    "objectID": "Day3Session3_github.html#add-files-to-repository-on-github",
    "href": "Day3Session3_github.html#add-files-to-repository-on-github",
    "title": "Version control: git and Github",
    "section": "Add files to repository on Github",
    "text": "Add files to repository on Github\nYou can add files to the repository by clicking on the button with the plus, +. You can either click Create new file and edit it on the Github page directly, or Upload files from your computer.\n\n\n\n\n\n\nNote\n\n\n\nWhenever you save a file on Github, it automatically creates a commit that will be part of the repository history.",
    "crumbs": [
      "Home",
      "Data management",
      "Version control: git and Github"
    ]
  },
  {
    "objectID": "Day3Session3_github.html#add-files-on-your-local-computer",
    "href": "Day3Session3_github.html#add-files-on-your-local-computer",
    "title": "Version control: git and Github",
    "section": "Add files on your local computer",
    "text": "Add files on your local computer\nTo change the content of the repository on your local computer you need to copy the repository content to your local computer. This is called cloning.\n\n\n\nInterface to clone a repository\n\n\n\nClick on the green Code button and select HTTPS on the Local tab.\nCopy the URL to the clipboard.\nOpen VScode and type `&gt; Git: Clone into the search bar.\nCopy in the URL you just copied from Github.\nFollow the prompts to select a location for the directory on your computer.\nOpen the repository with VScode.\n\nNow you can add files, edit and execute your code. Once you are satisfied with the changes you can stage, commit and push them.\n\n\n\n\n\n\nTip\n\n\n\nYou can now for example add a quarto homepage to your repository, so you have an interactive document to run and publish your analyses. Have a look at the quarto section, Creating a Project of this course to see how.",
    "crumbs": [
      "Home",
      "Data management",
      "Version control: git and Github"
    ]
  },
  {
    "objectID": "Day3Session3_github.html#stage-commit-and-push",
    "href": "Day3Session3_github.html#stage-commit-and-push",
    "title": "Version control: git and Github",
    "section": "Stage, commit, and push",
    "text": "Stage, commit, and push\nTo add your changes to the repository history you can either use the command line in the terminal of VScode, or the shortcut on the side panel on the left of the VScode window that looks like three circles that are connected by two wiggly lines:\n\n\n\nThe source control button in the sidebar\n\n\nHere is a little reminder of what we are trying to achieve:\n\n\n\nBasic git commands\n\n\n\nvia the command line\nIn the terminal of VScode, check the status of your project with git status.\n\n\n\ngit status example\n\n\nThere are a few files that are already tracked by git, but that have untracked changes, and a few files that are not tracked. (There is also one commit that is not yet pushed to the remote repository.) At the bottom it even says which commands to use to stage and commit changes.\nNow, I can stage the files with git add &lt;filename&gt;:\n\n\n\ngit add example\n\n\nNow we can see in green the changes that are to be committed. I can commit them with `git commit -m “&gt;commit message&gt;”:\n\n\n\ngit commit example\n\n\nThe commit is then incorporated into the repository, and briefly summarized in the command line. I can now use git push to push the changes to the remote repository on Github and publish them.\n\n\nvia VScode\nClicking on the git shortcut button in VScode opens a pane that shows which changes have been made.\n\n\n\nVScode status example\n\n\nBy clicking on the plus to the right of the file you can stage the file, and then commit them with the blue commit button and a commit message. You can also commit and push at the same time (by selecting that option in the drop down menu of the blue commit button).\n\n\n\nVScode staging example\n\n\n\n\n\n\n\n\nNote\n\n\n\nA commit message should always be informative. They will be the identifiers if you want to “go back in time” and revert to an earlier version of your repository.",
    "crumbs": [
      "Home",
      "Data management",
      "Version control: git and Github"
    ]
  },
  {
    "objectID": "Day3Session3_github.html#using-git-collaboratively",
    "href": "Day3Session3_github.html#using-git-collaboratively",
    "title": "Version control: git and Github",
    "section": "using git collaboratively",
    "text": "using git collaboratively\nWe have now introduced how you can use git and github by yourself. However, it is a very poweful tool for working together with others. Unfortunately, getting into the details of that would exceed the time we have in this course.\nHere are some resources for you to look into for further reading and training:\nGetting started with git.\nThe git book.\nVersion control with git from the software carpentries.\nWorking with GitHub in VScode.",
    "crumbs": [
      "Home",
      "Data management",
      "Version control: git and Github"
    ]
  },
  {
    "objectID": "Day3Session4_nfcore_main.html",
    "href": "Day3Session4_nfcore_main.html",
    "title": "Examining main.nf",
    "section": "",
    "text": "In this session, we will go through the main.nf file that actually executes the whole pipeline of the rnaseq pipeline we ran on Tuesday. We hope that this session will make the whole process less of a “black box” and more approachable.\nYou can find the main.nf file on GitHub\nOne of us will act as a scribe during the session, and record our most important discussion points here for you to look at later.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Examining main.nf"
    ]
  }
]